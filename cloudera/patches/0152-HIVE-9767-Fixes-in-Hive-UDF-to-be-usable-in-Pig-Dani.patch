From ca4263ec14700a776889396b2dddf7ac28c31c9a Mon Sep 17 00:00:00 2001
From: Jianyong Dai <daijy@apache.org>
Date: Thu, 26 Mar 2015 17:11:15 +0000
Subject: [PATCH 152/275] HIVE-9767: Fixes in Hive UDF to be usable in Pig
 (Daniel Dai)

git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1669371 13f79535-47bb-0310-9956-ffa450edef68
---
 .../ql/udf/generic/GenericUDAFComputeStats.java    |  160 ++++++++++----------
 .../ql/udf/generic/GenericUDAFContextNGrams.java   |   17 ++-
 .../hive/ql/udf/generic/GenericUDAFCount.java      |    5 +-
 .../hive/ql/udf/generic/GenericUDAFEWAHBitmap.java |    4 +-
 .../udf/generic/GenericUDAFHistogramNumeric.java   |   11 +-
 .../generic/GenericUDAFMkCollectionEvaluator.java  |    7 +-
 .../udf/generic/GenericUDAFPercentileApprox.java   |   10 +-
 .../hive/ql/udf/generic/GenericUDAFnGrams.java     |   11 +-
 .../hadoop/hive/ql/udf/generic/GenericUDFCase.java |    2 +-
 .../hadoop/hive/ql/udf/generic/GenericUDFIn.java   |    2 +-
 .../hive/ql/udf/generic/GenericUDFNamedStruct.java |    9 +-
 .../hive/ql/udf/generic/GenericUDFUtils.java       |   23 ++-
 .../hive/ql/udf/generic/GenericUDTFStack.java      |    7 +-
 .../hadoop/hive/ql/udf/generic/NGramEstimator.java |    5 +-
 .../hive/ql/udf/generic/NumericHistogram.java      |   14 +-
 15 files changed, 154 insertions(+), 133 deletions(-)

diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java
index 9702529..363039b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java
@@ -138,15 +138,15 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters)
       } else {
         soi = (StructObjectInspector) parameters[0];
 
-        countTruesField = soi.getStructFieldRef("CountTrues");
+        countTruesField = soi.getStructFieldRef("counttrues");
         countTruesFieldOI = (WritableLongObjectInspector)
                                countTruesField.getFieldObjectInspector();
 
-        countFalsesField = soi.getStructFieldRef("CountFalses");
+        countFalsesField = soi.getStructFieldRef("countfalses");
         countFalsesFieldOI = (WritableLongObjectInspector)
                                 countFalsesField.getFieldObjectInspector();
 
-        countNullsField = soi.getStructFieldRef("CountNulls");
+        countNullsField = soi.getStructFieldRef("countnulls");
         countNullsFieldOI = (WritableLongObjectInspector) countNullsField.getFieldObjectInspector();
       }
 
@@ -158,10 +158,10 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters)
       foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
 
       List<String> fname = new ArrayList<String>();
-      fname.add("ColumnType");
-      fname.add("CountTrues");
-      fname.add("CountFalses");
-      fname.add("CountNulls");
+      fname.add("columntype");
+      fname.add("counttrues");
+      fname.add("countfalses");
+      fname.add("countnulls");
 
       partialResult = new Object[4];
       partialResult[0] = new Text();
@@ -320,13 +320,13 @@ public Object terminate(AggregationBuffer agg) throws HiveException {
     protected transient OI maxFieldOI;
 
     protected transient StructField countNullsField;
-    protected transient WritableLongObjectInspector countNullsFieldOI;
+    protected transient LongObjectInspector countNullsFieldOI;
 
     protected transient StructField ndvField;
-    protected transient WritableStringObjectInspector ndvFieldOI;
+    protected transient StringObjectInspector ndvFieldOI;
 
     protected transient StructField numBitVectorsField;
-    protected transient WritableIntObjectInspector numBitVectorsFieldOI;
+    protected transient IntObjectInspector numBitVectorsFieldOI;
 
     /* Partial aggregation result returned by TerminatePartial. Partial result is a struct
      * containing a long field named "count".
@@ -352,20 +352,20 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc
       } else {
         soi = (StructObjectInspector) parameters[0];
 
-        minField = soi.getStructFieldRef("Min");
+        minField = soi.getStructFieldRef("min");
         minFieldOI = (OI) minField.getFieldObjectInspector();
 
-        maxField = soi.getStructFieldRef("Max");
+        maxField = soi.getStructFieldRef("max");
         maxFieldOI = (OI) maxField.getFieldObjectInspector();
 
-        countNullsField = soi.getStructFieldRef("CountNulls");
-        countNullsFieldOI = (WritableLongObjectInspector) countNullsField.getFieldObjectInspector();
+        countNullsField = soi.getStructFieldRef("countnulls");
+        countNullsFieldOI = (LongObjectInspector) countNullsField.getFieldObjectInspector();
 
-        ndvField = soi.getStructFieldRef("BitVector");
-        ndvFieldOI = (WritableStringObjectInspector) ndvField.getFieldObjectInspector();
+        ndvField = soi.getStructFieldRef("bitvector");
+        ndvFieldOI = (StringObjectInspector) ndvField.getFieldObjectInspector();
 
-        numBitVectorsField = soi.getStructFieldRef("NumBitVectors");
-        numBitVectorsFieldOI = (WritableIntObjectInspector)
+        numBitVectorsField = soi.getStructFieldRef("numbitvectors");
+        numBitVectorsFieldOI = (IntObjectInspector)
             numBitVectorsField.getFieldObjectInspector();
       }
 
@@ -380,12 +380,12 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc
         foi.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
 
         List<String> fname = new ArrayList<String>();
-        fname.add("ColumnType");
-        fname.add("Min");
-        fname.add("Max");
-        fname.add("CountNulls");
-        fname.add("BitVector");
-        fname.add("NumBitVectors");
+        fname.add("columnType");
+        fname.add("min");
+        fname.add("max");
+        fname.add("countnulls");
+        fname.add("bitvector");
+        fname.add("numbitvectors");
 
         partialResult = new Object[6];
         partialResult[0] = new Text();
@@ -404,11 +404,11 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc
         foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
 
         List<String> fname = new ArrayList<String>();
-        fname.add("ColumnType");
-        fname.add("Min");
-        fname.add("Max");
-        fname.add("CountNulls");
-        fname.add("NumDistinctValues");
+        fname.add("columnType");
+        fname.add("min");
+        fname.add("max");
+        fname.add("countnulls");
+        fname.add("numdistinctvalues");
 
         result = new Object[5];
         result[0] = new Text();
@@ -706,22 +706,22 @@ public void reset(AggregationBuffer agg) throws HiveException {
     private transient StructObjectInspector soi;
 
     private transient StructField maxLengthField;
-    private transient WritableLongObjectInspector maxLengthFieldOI;
+    private transient LongObjectInspector maxLengthFieldOI;
 
     private transient StructField sumLengthField;
-    private transient WritableLongObjectInspector sumLengthFieldOI;
+    private transient LongObjectInspector sumLengthFieldOI;
 
     private transient StructField countField;
-    private transient WritableLongObjectInspector countFieldOI;
+    private transient LongObjectInspector countFieldOI;
 
     private transient StructField countNullsField;
-    private transient WritableLongObjectInspector countNullsFieldOI;
+    private transient LongObjectInspector countNullsFieldOI;
 
     private transient StructField ndvField;
-    private transient WritableStringObjectInspector ndvFieldOI;
+    private transient StringObjectInspector ndvFieldOI;
 
     private transient StructField numBitVectorsField;
-    private transient WritableIntObjectInspector numBitVectorsFieldOI;
+    private transient IntObjectInspector numBitVectorsFieldOI;
 
     /* Output of final result of the aggregation
      */
@@ -738,23 +738,23 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc
       } else {
         soi = (StructObjectInspector) parameters[0];
 
-        maxLengthField = soi.getStructFieldRef("MaxLength");
-        maxLengthFieldOI = (WritableLongObjectInspector) maxLengthField.getFieldObjectInspector();
+        maxLengthField = soi.getStructFieldRef("maxlength");
+        maxLengthFieldOI = (LongObjectInspector) maxLengthField.getFieldObjectInspector();
 
-        sumLengthField = soi.getStructFieldRef("SumLength");
-        sumLengthFieldOI = (WritableLongObjectInspector) sumLengthField.getFieldObjectInspector();
+        sumLengthField = soi.getStructFieldRef("sumlength");
+        sumLengthFieldOI = (LongObjectInspector) sumLengthField.getFieldObjectInspector();
 
-        countField = soi.getStructFieldRef("Count");
-        countFieldOI = (WritableLongObjectInspector) countField.getFieldObjectInspector();
+        countField = soi.getStructFieldRef("count");
+        countFieldOI = (LongObjectInspector) countField.getFieldObjectInspector();
 
-        countNullsField = soi.getStructFieldRef("CountNulls");
-        countNullsFieldOI = (WritableLongObjectInspector) countNullsField.getFieldObjectInspector();
+        countNullsField = soi.getStructFieldRef("countnulls");
+        countNullsFieldOI = (LongObjectInspector) countNullsField.getFieldObjectInspector();
 
-        ndvField = soi.getStructFieldRef("BitVector");
-        ndvFieldOI = (WritableStringObjectInspector) ndvField.getFieldObjectInspector();
+        ndvField = soi.getStructFieldRef("bitvector");
+        ndvFieldOI = (StringObjectInspector) ndvField.getFieldObjectInspector();
 
-        numBitVectorsField = soi.getStructFieldRef("NumBitVectors");
-        numBitVectorsFieldOI = (WritableIntObjectInspector)
+        numBitVectorsField = soi.getStructFieldRef("numbitvectors");
+        numBitVectorsFieldOI = (IntObjectInspector)
                                   numBitVectorsField.getFieldObjectInspector();
       }
 
@@ -770,13 +770,13 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc
         foi.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
 
         List<String> fname = new ArrayList<String>();
-        fname.add("ColumnType");
-        fname.add("MaxLength");
-        fname.add("SumLength");
-        fname.add("Count");
-        fname.add("CountNulls");
-        fname.add("BitVector");
-        fname.add("NumBitVectors");
+        fname.add("columntype");
+        fname.add("maxlength");
+        fname.add("sumlength");
+        fname.add("count");
+        fname.add("countnulls");
+        fname.add("bitvector");
+        fname.add("numbitvectors");
 
         partialResult = new Object[7];
         partialResult[0] = new Text();
@@ -798,11 +798,11 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc
         foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
 
         List<String> fname = new ArrayList<String>();
-        fname.add("ColumnType");
-        fname.add("MaxLength");
-        fname.add("AvgLength");
-        fname.add("CountNulls");
-        fname.add("NumDistinctValues");
+        fname.add("columntype");
+        fname.add("maxlength");
+        fname.add("avglength");
+        fname.add("countnulls");
+        fname.add("numdistinctvalues");
 
         result = new Object[5];
         result[0] = new Text();
@@ -1030,16 +1030,16 @@ public Object terminate(AggregationBuffer agg) throws HiveException {
     private transient StructObjectInspector soi;
 
     private transient StructField maxLengthField;
-    private transient WritableLongObjectInspector maxLengthFieldOI;
+    private transient LongObjectInspector maxLengthFieldOI;
 
     private transient StructField sumLengthField;
-    private transient WritableLongObjectInspector sumLengthFieldOI;
+    private transient LongObjectInspector sumLengthFieldOI;
 
     private transient StructField countField;
-    private transient WritableLongObjectInspector countFieldOI;
+    private transient LongObjectInspector countFieldOI;
 
     private transient StructField countNullsField;
-    private transient WritableLongObjectInspector countNullsFieldOI;
+    private transient LongObjectInspector countNullsFieldOI;
 
     /* Output of final result of the aggregation
      */
@@ -1055,17 +1055,17 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc
       } else {
         soi = (StructObjectInspector) parameters[0];
 
-        maxLengthField = soi.getStructFieldRef("MaxLength");
-        maxLengthFieldOI = (WritableLongObjectInspector) maxLengthField.getFieldObjectInspector();
+        maxLengthField = soi.getStructFieldRef("maxlength");
+        maxLengthFieldOI = (LongObjectInspector) maxLengthField.getFieldObjectInspector();
 
-        sumLengthField = soi.getStructFieldRef("SumLength");
-        sumLengthFieldOI = (WritableLongObjectInspector) sumLengthField.getFieldObjectInspector();
+        sumLengthField = soi.getStructFieldRef("sumlength");
+        sumLengthFieldOI = (LongObjectInspector) sumLengthField.getFieldObjectInspector();
 
-        countField = soi.getStructFieldRef("Count");
-        countFieldOI = (WritableLongObjectInspector) countField.getFieldObjectInspector();
+        countField = soi.getStructFieldRef("count");
+        countFieldOI = (LongObjectInspector) countField.getFieldObjectInspector();
 
-        countNullsField = soi.getStructFieldRef("CountNulls");
-        countNullsFieldOI = (WritableLongObjectInspector) countNullsField.getFieldObjectInspector();
+        countNullsField = soi.getStructFieldRef("countnulls");
+        countNullsFieldOI = (LongObjectInspector) countNullsField.getFieldObjectInspector();
 
       }
 
@@ -1079,11 +1079,11 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc
         foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
 
         List<String> fname = new ArrayList<String>();
-        fname.add("ColumnType");
-        fname.add("MaxLength");
-        fname.add("SumLength");
-        fname.add("Count");
-        fname.add("CountNulls");
+        fname.add("columntype");
+        fname.add("maxlength");
+        fname.add("sumlength");
+        fname.add("count");
+        fname.add("countnulls");
 
         partialResult = new Object[5];
         partialResult[0] = new Text();
@@ -1102,10 +1102,10 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc
         foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
 
         List<String> fname = new ArrayList<String>();
-        fname.add("ColumnType");
-        fname.add("MaxLength");
-        fname.add("AvgLength");
-        fname.add("CountNulls");
+        fname.add("columntype");
+        fname.add("maxlength");
+        fname.add("avglength");
+        fname.add("countnulls");
 
         result = new Object[4];
         result[0] = new Text();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFContextNGrams.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFContextNGrams.java
index 17e9d76..49e3dcf 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFContextNGrams.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFContextNGrams.java
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
@@ -158,16 +159,16 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
    */
   public static class GenericUDAFContextNGramEvaluator extends GenericUDAFEvaluator {
     // For PARTIAL1 and COMPLETE: ObjectInspectors for original data
-    private transient StandardListObjectInspector outerInputOI;
+    private transient ListObjectInspector outerInputOI;
     private transient StandardListObjectInspector innerInputOI;
-    private transient StandardListObjectInspector contextListOI;
+    private transient ListObjectInspector contextListOI;
     private PrimitiveObjectInspector contextOI;
     private PrimitiveObjectInspector inputOI;
     private transient PrimitiveObjectInspector kOI;
     private transient PrimitiveObjectInspector pOI;
 
     // For PARTIAL2 and FINAL: ObjectInspectors for partial aggregations
-    private transient StandardListObjectInspector loi;
+    private transient ListObjectInspector loi;
 
     @Override
     public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException {
@@ -175,7 +176,7 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc
 
       // Init input object inspectors
       if (m == Mode.PARTIAL1 || m == Mode.COMPLETE) {
-        outerInputOI = (StandardListObjectInspector) parameters[0];
+        outerInputOI = (ListObjectInspector) parameters[0];
         if(outerInputOI.getListElementObjectInspector().getCategory() ==
             ObjectInspector.Category.LIST) {
           // We're dealing with input that is an array of arrays of strings
@@ -186,7 +187,7 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc
           inputOI = (PrimitiveObjectInspector) outerInputOI.getListElementObjectInspector();
           innerInputOI = null;
         }
-        contextListOI = (StandardListObjectInspector) parameters[1];
+        contextListOI = (ListObjectInspector) parameters[1];
         contextOI = (PrimitiveObjectInspector) contextListOI.getListElementObjectInspector();
         kOI = (PrimitiveObjectInspector) parameters[2];
         if(parameters.length == 4) {
@@ -196,7 +197,7 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc
         }
       } else {
           // Init the list object inspector for handling partial aggregations
-          loi = (StandardListObjectInspector) parameters[0];
+          loi = (ListObjectInspector) parameters[0];
       }
 
       // Init output object inspectors.
@@ -229,10 +230,10 @@ public void merge(AggregationBuffer agg, Object obj) throws HiveException {
         return;
       }
       NGramAggBuf myagg = (NGramAggBuf) agg;
-      List<Text> partial = (List<Text>) loi.getList(obj);
+      List partial = (List) loi.getList(obj);
 
       // remove the context words from the end of the list
-      int contextSize = Integer.parseInt( ((Text)partial.get(partial.size()-1)).toString() );
+      int contextSize = Integer.parseInt( partial.get(partial.size()-1).toString() );
       partial.remove(partial.size()-1);
       if(myagg.context.size() > 0)  {
         if(contextSize != myagg.context.size()) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCount.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCount.java
index 54aa987..d47e7f9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCount.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCount.java
@@ -88,8 +88,9 @@ public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo paramInfo)
     public ObjectInspector init(Mode m, ObjectInspector[] parameters)
     throws HiveException {
       super.init(m, parameters);
-      partialCountAggOI =
-        PrimitiveObjectInspectorFactory.writableLongObjectInspector;
+      if (mode == Mode.PARTIAL2 || mode == Mode.FINAL) {
+        partialCountAggOI = (LongObjectInspector)parameters[0];
+      }
       result = new LongWritable(0);
       return PrimitiveObjectInspectorFactory.writableLongObjectInspector;
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFEWAHBitmap.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFEWAHBitmap.java
index e4b412e..22b8545 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFEWAHBitmap.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFEWAHBitmap.java
@@ -91,12 +91,12 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters)
             .getStandardListObjectInspector(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
       } else if (m == Mode.PARTIAL2 || m == Mode.FINAL) {
         internalMergeOI = (StandardListObjectInspector) parameters[0];
-        inputOI = PrimitiveObjectInspectorFactory.writableByteObjectInspector;
+        inputOI = (PrimitiveObjectInspector)internalMergeOI.getListElementObjectInspector();
         loi = (StandardListObjectInspector) ObjectInspectorFactory
             .getStandardListObjectInspector(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
         return loi;
       } else { // Mode.COMPLETE, ie. no map-side aggregation, requires ordering
-        inputOI = PrimitiveObjectInspectorFactory.writableByteObjectInspector;
+        inputOI = (PrimitiveObjectInspector)parameters[0];
         loi = (StandardListObjectInspector) ObjectInspectorFactory
             .getStandardListObjectInspector(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
         return loi;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFHistogramNumeric.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFHistogramNumeric.java
index f2e8e03..008d937 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFHistogramNumeric.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFHistogramNumeric.java
@@ -28,10 +28,11 @@
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
@@ -124,7 +125,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
     private transient PrimitiveObjectInspector nbinsOI;
 
     // For PARTIAL2 and FINAL: ObjectInspectors for partial aggregations (list of doubles)
-    private transient StandardListObjectInspector loi;
+    private transient ListObjectInspector loi;
 
 
     @Override
@@ -137,7 +138,7 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc
         inputOI = (PrimitiveObjectInspector) parameters[0];
         nbinsOI = (PrimitiveObjectInspector) parameters[1];
       } else {
-        loi = (StandardListObjectInspector) parameters[0];
+        loi = (ListObjectInspector) parameters[0];
       }
 
       // init output object inspectors
@@ -197,8 +198,10 @@ public void merge(AggregationBuffer agg, Object partial) throws HiveException {
         return;
       }
       List<DoubleWritable> partialHistogram = (List<DoubleWritable>) loi.getList(partial);
+      DoubleObjectInspector doi = (DoubleObjectInspector)loi.getListElementObjectInspector();
+      
       StdAgg myagg = (StdAgg) agg;
-      myagg.histogram.merge(partialHistogram);
+      myagg.histogram.merge(partialHistogram, doi);
     }
 
     @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFMkCollectionEvaluator.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFMkCollectionEvaluator.java
index d520e01..efcc8f5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFMkCollectionEvaluator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFMkCollectionEvaluator.java
@@ -25,6 +25,7 @@
 import java.util.List;
 
 import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
@@ -44,7 +45,7 @@
   // of objs)
   private transient StandardListObjectInspector loi;
 
-  private transient StandardListObjectInspector internalMergeOI;
+  private transient ListObjectInspector internalMergeOI;
 
   private BufferType bufferType;
 
@@ -68,14 +69,14 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters)
           .getStandardListObjectInspector((PrimitiveObjectInspector) ObjectInspectorUtils
               .getStandardObjectInspector(inputOI));
     } else {
-      if (!(parameters[0] instanceof StandardListObjectInspector)) {
+      if (!(parameters[0] instanceof ListObjectInspector)) {
         //no map aggregation.
         inputOI = (PrimitiveObjectInspector)  ObjectInspectorUtils
         .getStandardObjectInspector(parameters[0]);
         return (StandardListObjectInspector) ObjectInspectorFactory
             .getStandardListObjectInspector(inputOI);
       } else {
-        internalMergeOI = (StandardListObjectInspector) parameters[0];
+        internalMergeOI = (ListObjectInspector) parameters[0];
         inputOI = (PrimitiveObjectInspector) internalMergeOI.getListElementObjectInspector();
         loi = (StandardListObjectInspector) ObjectInspectorUtils.getStandardObjectInspector(internalMergeOI);
         return loi;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFPercentileApprox.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFPercentileApprox.java
index 56e76be..e53b893 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFPercentileApprox.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFPercentileApprox.java
@@ -35,6 +35,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
 
@@ -300,20 +301,21 @@ public void merge(AggregationBuffer agg, Object partial) throws HiveException {
         return;
       }
       PercentileAggBuf myagg = (PercentileAggBuf) agg;
-      List<DoubleWritable> partialHistogram = (List<DoubleWritable>) loi.getList(partial);
+      List partialHistogram = (List) loi.getList(partial);
+      DoubleObjectInspector doi = (DoubleObjectInspector)loi.getListElementObjectInspector();
 
       // remove requested quantiles from the head of the list
-      int nquantiles = (int) partialHistogram.get(0).get();
+      int nquantiles = (int) doi.get(partialHistogram.get(0));
       if(nquantiles > 0) {
         myagg.quantiles = new double[nquantiles];
         for(int i = 1; i <= nquantiles; i++) {
-          myagg.quantiles[i-1] = partialHistogram.get(i).get();
+          myagg.quantiles[i-1] = doi.get(partialHistogram.get(i));
         }
         partialHistogram.subList(0, nquantiles+1).clear();
       }
 
       // merge histograms
-      myagg.histogram.merge(partialHistogram);
+      myagg.histogram.merge(partialHistogram, doi);
     }
 
     @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFnGrams.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFnGrams.java
index 7de25ff..1c9456e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFnGrams.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFnGrams.java
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
@@ -160,7 +161,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
    */
   public static class GenericUDAFnGramEvaluator extends GenericUDAFEvaluator {
     // For PARTIAL1 and COMPLETE: ObjectInspectors for original data
-    private transient StandardListObjectInspector outerInputOI;
+    private transient ListObjectInspector outerInputOI;
     private transient StandardListObjectInspector innerInputOI;
     private transient PrimitiveObjectInspector inputOI;
     private transient PrimitiveObjectInspector nOI;
@@ -168,7 +169,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
     private transient PrimitiveObjectInspector pOI;
 
     // For PARTIAL2 and FINAL: ObjectInspectors for partial aggregations
-    private transient StandardListObjectInspector loi;
+    private transient ListObjectInspector loi;
 
     @Override
     public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException {
@@ -176,7 +177,7 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc
 
       // Init input object inspectors
       if (m == Mode.PARTIAL1 || m == Mode.COMPLETE) {
-        outerInputOI = (StandardListObjectInspector) parameters[0];
+        outerInputOI = (ListObjectInspector) parameters[0];
         if(outerInputOI.getListElementObjectInspector().getCategory() ==
             ObjectInspector.Category.LIST) {
           // We're dealing with input that is an array of arrays of strings
@@ -196,7 +197,7 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc
         }
       } else {
           // Init the list object inspector for handling partial aggregations
-          loi = (StandardListObjectInspector) parameters[0];
+          loi = (ListObjectInspector) parameters[0];
       }
 
       // Init output object inspectors.
@@ -229,7 +230,7 @@ public void merge(AggregationBuffer agg, Object partial) throws HiveException {
         return;
       }
       NGramAggBuf myagg = (NGramAggBuf) agg;
-      List<Text> partialNGrams = (List<Text>) loi.getList(partial);
+      List partialNGrams = (List) loi.getList(partial);
       int n = Integer.parseInt(partialNGrams.get(partialNGrams.size()-1).toString());
 
       // A value of 0 for n indicates that the mapper processed data that does not meet
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCase.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCase.java
index 07cc84c..f80be6b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCase.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCase.java
@@ -101,7 +101,7 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {
       PrimitiveObjectInspector caseOI = (PrimitiveObjectInspector) caseOIResolver.get();
       if (PrimitiveObjectInspectorUtils.comparePrimitiveObjects(
             caseOIResolver.convertIfNecessary(exprValue, argumentOIs[0]), caseOI,
-            caseOIResolver.convertIfNecessary(caseKey, argumentOIs[i]), caseOI)) {
+            caseOIResolver.convertIfNecessary(caseKey, argumentOIs[i], false), caseOI)) {
         Object caseValue = arguments[i + 1].get();
         return returnOIResolver.convertIfNecessary(caseValue, argumentOIs[i + 1]);
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFIn.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFIn.java
index 8990e1d..38b1dc4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFIn.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFIn.java
@@ -181,7 +181,7 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {
             conversionHelper.convertIfNecessary(
                 arguments[0].get(), argumentOIs[0]), compareOI,
             conversionHelper.convertIfNecessary(
-                arguments[i].get(), argumentOIs[i]), compareOI) == 0) {
+                arguments[i].get(), argumentOIs[i], false), compareOI) == 0) {
           bw.set(true);
           return bw;
         }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFNamedStruct.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFNamedStruct.java
index 7b18d82..48c85a8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFNamedStruct.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFNamedStruct.java
@@ -19,16 +19,15 @@
 package org.apache.hadoop.hive.ql.udf.generic;
 
 import java.util.ArrayList;
-import java.util.Arrays;
 
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde2.objectinspector.ConstantObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantStringObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 
 @Description(name = "named_struct",
@@ -51,12 +50,12 @@ public ObjectInspector initialize(ObjectInspector[] arguments)
     ArrayList<String> fname = new ArrayList<String>(numFields / 2);
     ArrayList<ObjectInspector> retOIs = new ArrayList<ObjectInspector>(numFields / 2);
     for (int f = 0; f < numFields; f+=2) {
-      if (!(arguments[f] instanceof WritableConstantStringObjectInspector)) {
+      if (!(arguments[f] instanceof ConstantObjectInspector)) {
         throw new UDFArgumentTypeException(f, "Even arguments" +
             " to NAMED_STRUCT must be a constant STRING." + arguments[f].toString());
       }
-      WritableConstantStringObjectInspector constantOI =
-        (WritableConstantStringObjectInspector)arguments[f];
+      ConstantObjectInspector constantOI =
+        (ConstantObjectInspector)arguments[f];
       fname.add(constantOI.getWritableConstantValue().toString());
       retOIs.add(arguments[f + 1]);
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java
index f197afa..09d2d1f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java
@@ -198,11 +198,17 @@ public ObjectInspector get(ObjectInspector defaultOI) {
       return returnObjectInspector != null ? returnObjectInspector : defaultOI;
     }
 
+    public Object convertIfNecessary(Object o, ObjectInspector oi) {
+      return convertIfNecessary(o, oi, true);
+    }
+
     /**
      * Convert the return Object if necessary (when the ObjectInspectors of
-     * different possibilities are not all the same).
+     * different possibilities are not all the same). If reuse is true, 
+     * the result Object will be the same object as the last invocation 
+     * (as long as the oi is the same)
      */
-    public Object convertIfNecessary(Object o, ObjectInspector oi) {
+    public Object convertIfNecessary(Object o, ObjectInspector oi, boolean reuse) {
       Object converted = null;
       if (oi == returnObjectInspector) {
         converted = o;
@@ -212,15 +218,20 @@ public Object convertIfNecessary(Object o, ObjectInspector oi) {
           return null;
         }
 
-        if (converters == null) {
-          converters = new HashMap<ObjectInspector, Converter>();
+        Converter converter = null;
+        if (reuse) {
+	  if (converters == null) {
+	    converters = new HashMap<ObjectInspector, Converter>();
+	  }
+	  converter = converters.get(oi);
         }
 
-        Converter converter = converters.get(oi);
         if (converter == null) {
           converter = ObjectInspectorConverters.getConverter(oi,
               returnObjectInspector);
-          converters.put(oi, converter);
+          if (reuse) {
+            converters.put(oi, converter);
+          }
         }
         converted = converter.convert(o);
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFStack.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFStack.java
index 1a744e0..a2a976c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFStack.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFStack.java
@@ -25,6 +25,7 @@
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.ReturnObjectInspectorResolver;
+import org.apache.hadoop.hive.serde2.objectinspector.ConstantObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
@@ -60,13 +61,13 @@ public StructObjectInspector initialize(ObjectInspector[] args)
     if (args.length < 2)  {
       throw new UDFArgumentException("STACK() expects at least two arguments.");
     }
-    if (!(args[0] instanceof WritableConstantIntObjectInspector)) {
+    if (!(args[0] instanceof ConstantObjectInspector)) {
       throw new UDFArgumentException(
           "The first argument to STACK() must be a constant integer (got " +
           args[0].getTypeName() + " instead).");
     }
-    numRows =
-        ((WritableConstantIntObjectInspector)args[0]).getWritableConstantValue();
+    numRows = (IntWritable)
+        ((ConstantObjectInspector)args[0]).getWritableConstantValue();
 
     if (numRows == null || numRows.get() < 1) {
       throw new UDFArgumentException(
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NGramEstimator.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NGramEstimator.java
index a199ec9..1424ba8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NGramEstimator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NGramEstimator.java
@@ -208,7 +208,7 @@ public int compare(Map.Entry<ArrayList<String>,Double> o1,
    *
    * @param other A serialized n-gram object created by the serialize() method
    */
-  public void merge(List<Text> other) throws HiveException {
+  public void merge(List other) throws HiveException {
     if(other == null) {
       return;
     }
@@ -240,8 +240,7 @@ public void merge(List<Text> other) throws HiveException {
     for(int i = 3; i < other.size(); i++) {
       ArrayList<String> key = new ArrayList<String>();
       for(int j = 0; j < n; j++) {
-        Text word = other.get(i+j);
-        key.add(word.toString());
+        key.add(other.get(i+j).toString());
       }
       i += n;
       double val = Double.parseDouble( other.get(i).toString() );
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java
index a73ee68..0577a4f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java
@@ -21,7 +21,9 @@
 import java.util.List;
 import java.util.Collections;
 import java.util.Random;
+
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
 
 
 /**
@@ -115,7 +117,7 @@ public void allocate(int num_bins) {
    * @param other A serialized histogram created by the serialize() method
    * @see #merge
    */
-  public void merge(List<DoubleWritable> other) {
+  public void merge(List other, DoubleObjectInspector doi) {
     if(other == null) {
       return;
     }
@@ -123,13 +125,13 @@ public void merge(List<DoubleWritable> other) {
     if(nbins == 0 || nusedbins == 0)  {
       // Our aggregation buffer has nothing in it, so just copy over 'other'
       // by deserializing the ArrayList of (x,y) pairs into an array of Coord objects
-      nbins = (int) other.get(0).get();
+      nbins = (int) doi.get(other.get(0));
       nusedbins = (other.size()-1)/2;
       bins = new ArrayList<Coord>(nusedbins);
       for (int i = 1; i < other.size(); i+=2) {
         Coord bin = new Coord();
-        bin.x = other.get(i).get();
-        bin.y = other.get(i+1).get();
+        bin.x = doi.get(other.get(i));
+        bin.y = doi.get(other.get(i+1));
         bins.add(bin);
       }
     } else {
@@ -146,8 +148,8 @@ public void merge(List<DoubleWritable> other) {
       }
       for (int j = 1; j < other.size(); j += 2) {
         Coord bin = new Coord();
-        bin.x = other.get(j).get();
-        bin.y = other.get(j+1).get();
+        bin.x = doi.get(other.get(j));
+        bin.y = doi.get(other.get(j+1));
         tmp_bins.add(bin);
       }
       Collections.sort(tmp_bins);
-- 
1.7.9.5

